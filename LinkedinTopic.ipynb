{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Linkedin Summaries using Topic Modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib as matplot\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "plt.style.use(\"ggplot\")\n",
    "matplotlib.rcParams.update({\"font.size\": 14})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "#Load the Google Data Scientist Linkedin Data\n",
    "googleDatasci = pd.read_csv(\"Datasets/googledatasci.csv\")\n",
    "\n",
    "googleDatasci['summary'] = googleDatasci['summary'].str.strip()\n",
    "\n",
    "#Remove the first column\n",
    "del googleDatasci['Unnamed: 0']\n",
    "\n",
    "#Select only the rows with Google as company\n",
    "googleDatasci = googleDatasci.loc[(googleDatasci.company == 'Google')]\n",
    "\n",
    "#Remove any row that doesn't contain data scientist in the headline\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"Technical Program Manager\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"Supply chain & Operations leader\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"DevIntel on Developer Products\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"China Display Product Lead\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"Sales Operations Senior\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"APM Intern\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"Trust and Safety manager\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"User Growth, Google Play\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"Ads Solutions Consultant\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"Regional Customer Engineer\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"Software Engineering Intern\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"Product Manager\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"Software Developer\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"Software Engineer\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"Software engineer\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"Opportunity Seeker\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"Student at University of Toronto\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"Google Policy Fellow\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"Science Advocate\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"Engineer\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"Account Manager Display\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"Customer Engineering Manager\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"Strategist\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"Program Manager\") == False]\n",
    "\n",
    "#print out the dataset\n",
    "googleDatasci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Remove any unnecessary characters \n",
    "googleDatasci['summary'] = googleDatasci['summary'].replace(r'\\t', '', regex=True) \n",
    "googleDatasci['summary'] = googleDatasci['summary'].replace(r'\\n', ' ', regex=True) \n",
    "googleDatasci['summary'] = googleDatasci['summary'].replace(r'\\...', ' ', regex=True)\n",
    "googleDatasci['summary'] = googleDatasci['summary'].replace(r'\\..', ' ', regex=True)\n",
    "googleDatasci['summary'] = googleDatasci['summary'].replace('●', '', regex=True) \n",
    "googleDatasci['summary'] = googleDatasci['summary'].replace('•', '', regex=True)\n",
    "googleDatasci['summary'] = googleDatasci['summary'].replace(r'\\-', '', regex=True) \n",
    "googleDatasci['summary'] = googleDatasci['summary'].replace(r'\\&', '', regex=True)\n",
    "googleDatasci['summary'] = googleDatasci['summary'].replace(r'\\$', '', regex=True)\n",
    "googleDatasci['summary'] = googleDatasci['summary'].replace(',', '', regex=True)\n",
    "googleDatasci['summary'] = googleDatasci['summary'].replace('\\d+', '', regex=True)\n",
    "googleDatasci['summary'] = googleDatasci['summary'].replace(r'\\(','', regex=True)\n",
    "googleDatasci['summary'] = googleDatasci['summary'].replace(r'\\)','', regex=True)\n",
    "googleDatasci['summary'] = googleDatasci['summary'].replace(r'\\/','', regex=True)\n",
    "googleDatasci['summary'] = googleDatasci['summary'].replace(r'\\:','', regex=True) \n",
    "googleDatasci['summary'] = googleDatasci['summary'].replace(r'\\–','', regex=True)\n",
    "googleDatasci['summary'] = googleDatasci['summary'].replace(r'\\%','', regex=True)\n",
    "googleDatasci['summary'] = googleDatasci['summary'].replace(r'\\*','', regex=True)\n",
    "googleDatasci['summary'] = googleDatasci['summary'].replace(r'\\@','', regex=True)\n",
    "googleDatasci['summary'] = googleDatasci['summary'].replace(r'\\_________','', regex=True)\n",
    "googleDatasci['summary'] = googleDatasci['summary'].replace(r'\\____','', regex=True)\n",
    "googleDatasci['summary'] = googleDatasci['summary'].replace(r'\\!','', regex=True)\n",
    "googleDatasci['summary'] = googleDatasci['summary'].replace(r'\\®','', regex=True)\n",
    "googleDatasci['summary'] = googleDatasci['summary'].replace(r'\\>','', regex=True)\n",
    "googleDatasci['summary'] = googleDatasci['summary'].replace(r'\\?','', regex=True)\n",
    "googleDatasci['summary'] = googleDatasci['summary'].replace(r'\\;','', regex=True)\n",
    "googleDatasci['summary'] = googleDatasci['summary'].replace(r'\\=','', regex=True)\n",
    "\n",
    "#googleDatasci = googleDatasci.translate(None, \"(){}<>\")\n",
    "\n",
    "#\n",
    "googleDatasci['summary'] = googleDatasci['summary'].str.lower()\n",
    "\n",
    "#Remove any row with a NaN\n",
    "#googleDatasci['summary'] = googleDatasci['summary'].dropna()\n",
    "googleDatasci = googleDatasci[googleDatasci.summary.str.contains(\"NaN\") == False]\n",
    "\n",
    "\n",
    "googleDatasci\n",
    "\n",
    "#gs = googleDatasci['summary']\n",
    "\n",
    "#googleSummary = pd.DataFrame(gs)\n",
    "#googleSummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "ws_tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "article = googleDatasci['summary'][1]\n",
    "\n",
    "# tokenize example document\n",
    "nyt_ws_tokens = ws_tokenizer.tokenize(article.lower())\n",
    "\n",
    "#print (nyt_ws_tokens)\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Weighting using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "googleDatasci['summary'].to_csv('summary.txt', index=False, sep=' ', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# we can pass in the same preprocessing parameters\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "raw_documents = []\n",
    "snippets = []\n",
    "with open(\"summary.txt\",\"r\") as fin:\n",
    "    for line in fin.readlines():\n",
    "        text = line.strip()\n",
    "        raw_documents.append( text )\n",
    "        # keep a short snippet of up to 100 characters as a title for each article\n",
    "        snippets.append( text[0:min(len(text),100)] )\n",
    "print(\"Read %d raw text documents\" % len(raw_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words, max_df = 20)\n",
    "A = vectorizer.fit_transform(raw_documents)\n",
    "print( \"Created %d X %d TF-IDF-normalized document-term matrix\" % (A.shape[0], A.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the resulting vocabulary\n",
    "terms = vectorizer.get_feature_names()\n",
    "print(\"Vocabulary has %d distinct terms\" % len(terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "def rank_terms( A, terms ):\n",
    "    # get the sums over each column\n",
    "    sums = A.sum(axis=0)\n",
    "    # map weights to the terms\n",
    "    weights = {}\n",
    "    for col, term in enumerate(terms):\n",
    "        weights[term] = sums[0,col]\n",
    "    # rank the terms by their weight over all documents\n",
    "    return sorted(weights.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking = rank_terms( A, terms )\n",
    "for i, pair in enumerate( ranking[0:20] ):\n",
    "    print( \"%02d. %s (%.2f)\" % ( i+1, pair[0], pair[1] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Topic Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input parameter for the number of topics to make\n",
    "k = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the model\n",
    "from sklearn import decomposition\n",
    "model = decomposition.NMF( init=\"nndsvd\", n_components=k ) \n",
    "# apply the model and extract the two factor matrices\n",
    "W = model.fit_transform( A )\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See the output of the model\n",
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round to 2 decimal places for display purposes# round \n",
    "W[0,:].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See the H factor\n",
    "H.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is science associated with\n",
    "term_index = terms.index('learning')\n",
    "# round to 2 decimal places for display purposes\n",
    "H[:,term_index].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Topic Descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_descriptor( terms, H, topic_index, top ):\n",
    "    # reverse sort the values to sort the indices\n",
    "    top_indices = np.argsort( H[topic_index,:] )[::-1]\n",
    "    # now get the terms corresponding to the top-ranked indices\n",
    "    top_terms = []\n",
    "    for term_index in top_indices[0:top]:\n",
    "        top_terms.append( terms[term_index] )\n",
    "    return top_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors = []\n",
    "for topic_index in range(k):\n",
    "    descriptors.append( get_descriptor( terms, H, topic_index, 10 ) )\n",
    "    str_descriptor = \", \".join( descriptors[topic_index] )\n",
    "    print(\"Topic %02d: %s\" % ( topic_index+1, str_descriptor ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_top_term_weights( terms, H, topic_index, top ):\n",
    "    # get the top terms and their weights\n",
    "    top_indices = np.argsort( H[topic_index,:] )[::-1]\n",
    "    top_terms = []\n",
    "    top_weights = []\n",
    "    for term_index in top_indices[0:top]:\n",
    "        top_terms.append( terms[term_index] )\n",
    "        top_weights.append( H[topic_index,term_index] )\n",
    "    # note we reverse the ordering for the plot\n",
    "    top_terms.reverse()\n",
    "    top_weights.reverse()\n",
    "    # create the plot\n",
    "    fig = plt.figure(figsize=(13,8))\n",
    "    # add the horizontal bar chart\n",
    "    ypos = np.arange(top)\n",
    "    ax = plt.barh(ypos, top_weights, align=\"center\", color=\"green\",tick_label=top_terms)\n",
    "    plt.xlabel(\"Term Weight\",fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_top_term_weights( terms, H, 0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top_snippets( all_snippets, W, topic_index, top ):\n",
    "    # reverse sort the values to sort the indices\n",
    "    top_indices = np.argsort( W[:,topic_index] )[::-1]\n",
    "    # now get the snippets corresponding to the top-ranked indices\n",
    "    top_snippets = []\n",
    "    for doc_index in top_indices[0:top]:\n",
    "        top_snippets.append( all_snippets[doc_index] )\n",
    "    return top_snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_snippets = get_top_snippets( snippets, W, 1, 10 )\n",
    "for i, snippet in enumerate(topic_snippets):\n",
    "    print(\"%02d. %s\" % ( (i+1), snippet ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_snippets = get_top_snippets( snippets, W, 2, 10 )\n",
    "for i, snippet in enumerate(topic_snippets):\n",
    "    print(\"%02d. %s\" % ( (i+1), snippet ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump((W,H,terms,snippets), \"articles-model-nmf-k%02d.pkl\" % k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Topic Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initial range of topic models\n",
    "kmin, kmax = 4, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_models = []\n",
    "# try each value of k\n",
    "for k in range(kmin,kmax+1):\n",
    "    print(\"Applying NMF for k=%d ...\" % k )\n",
    "    # run NMF\n",
    "    model = decomposition.NMF( init=\"nndsvd\", n_components=k ) \n",
    "    W = model.fit_transform( A )\n",
    "    H = model.components_    \n",
    "    # store for later\n",
    "    topic_models.append( (k,W,H) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "class TokenGenerator:\n",
    "    def __init__( self, documents, stopwords ):\n",
    "        self.documents = documents\n",
    "        self.stopwords = stopwords\n",
    "        self.tokenizer = re.compile( r\"(?u)\\b\\w\\w+\\b\" )\n",
    "\n",
    "    def __iter__( self ):\n",
    "        print(\"Building Word2Vec model ...\")\n",
    "        for doc in self.documents:\n",
    "            tokens = []\n",
    "            for tok in self.tokenizer.findall( doc ):\n",
    "                if tok in self.stopwords:\n",
    "                    tokens.append( \"<stopword>\" )\n",
    "                elif len(tok) >= 2:\n",
    "                    tokens.append( tok )\n",
    "            yield tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "docgen = TokenGenerator(raw_documents, stop_words)\n",
    "# the model has 500 dimensions, the minimum document-term frequency is 20\n",
    "w2v_model = gensim.models.Word2Vec(docgen, size=1, min_count=1, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"Model has %d terms\" % len(w2v_model.wv.vocab) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_model.save(\"w2v-model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_coherence( w2v_model, term_rankings ):\n",
    "    overall_coherence = 0.0\n",
    "    for topic_index in range(len(term_rankings)):\n",
    "        # check each pair of terms\n",
    "        pair_scores = []\n",
    "        for pair in combinations( term_rankings[topic_index], 2 ):\n",
    "            pair_scores.append( w2v_model.similarity(pair[0], pair[1]) )\n",
    "        # get the mean for all pairs in this topic\n",
    "        topic_score = sum(pair_scores) / len(pair_scores)\n",
    "        overall_coherence += topic_score\n",
    "    # get the mean score across all topics\n",
    "    return overall_coherence / len(term_rankings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_descriptor( all_terms, H, topic_index, top ):\n",
    "    # reverse sort the values to sort the indices\n",
    "    top_indices = np.argsort( H[topic_index,:] )[::-1]\n",
    "    # now get the terms corresponding to the top-ranked indices\n",
    "    top_terms = []\n",
    "    for term_index in top_indices[0:top]:\n",
    "        top_terms.append( all_terms[term_index] )\n",
    "    return top_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "k_values = []\n",
    "coherences = []\n",
    "for (k,W,H) in topic_models:\n",
    "    # Get all of the topic descriptors - the term_rankings, based on top 10 terms\n",
    "    term_rankings = []\n",
    "    for topic_index in range(k):\n",
    "        term_rankings.append( get_descriptor( terms, H, topic_index, 10 ) )\n",
    "    # Now calculate the coherence based on our Word2vec model\n",
    "    k_values.append( k )\n",
    "    coherences.append( calculate_coherence( w2v_model, term_rankings ) )\n",
    "    print(\"K=%02d: Coherence=%.4f\" % ( k, coherences[-1] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Select only the rows with Google as company\n",
    "googleDatasci = googleDatasci.loc[(googleDatasci.company == 'Google')]\n",
    "\n",
    "#Remove any row that doesn't contain data scientist in the headline\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"Technical Program Manager\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"Supply chain & Operations leader\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"DevIntel on Developer Products\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"China Display Product Lead\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"Sales Operations Senior\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"APM Intern\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"Trust and Safety manager\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"User Growth, Google Play\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"Ads Solutions Consultant\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"Regional Customer Engineer\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"Software Engineering Intern\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"Product Manager\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"Software Developer\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"Software Engineer\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"Software engineer\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"Opportunity Seeker\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"Student at University of Toronto\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"Google Policy Fellow\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"Science Advocate\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"Engineer\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"Account Manager Display\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"Customer Engineering Manager\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"Strategist\") == False]\n",
    "googleDatasci = googleDatasci[googleDatasci.headline.str.contains(\"Program Manager\") == False]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
